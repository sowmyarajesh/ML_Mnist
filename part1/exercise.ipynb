{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from linear_regression import *\n",
    "from softmax import *\n",
    "from features import *\n",
    "from kernel import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#######################################################################\n",
    "# 1. Introduction\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data:\n",
    "train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the linear regression problem, you recall the linear regression has a closed form solution:\n",
    "\n",
    "```\n",
    "    theta = ((transpose(X).X+lambda* I) ^ -1). transpose(X).Y\n",
    "```\n",
    "\n",
    "where I = identity matrix\n",
    "\n",
    "lambda = regularization parameter\n",
    "\n",
    "X = input feature\n",
    "\n",
    "Y = output label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IdentityMatrix(n):\n",
    "    imat = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if r==c:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        imat.append(row)\n",
    "    return imat\n",
    "    \n",
    "def DiagonalMatrix(n,l):\n",
    "    dmat = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if r==c:\n",
    "                row.append(l)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        dmat.append(row)\n",
    "    return dmat\n",
    "def H_Func(n):\n",
    "    one = np.ones(n)\n",
    "    i = IdentityMatrix(n)\n",
    "    H = np.subtract(i, np.dot(1/n, np.matmul(one, np.transpose(one))))\n",
    "    return H\n",
    "\n",
    "def gramMatrix(vectorList):\n",
    "    V = np.array(vectorList)\n",
    "    G = V.dot(V.T)\n",
    "    return G\n",
    "\n",
    "def eigenValues(vector):\n",
    "    w,v  = np.linalg.eig(vector)\n",
    "    return np.round(w,3)\n",
    "\n",
    "def eigenVector(vector):\n",
    "    w,v  = np.linalg.eig(vector)\n",
    "    return np.round(v,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "    # create Identity matrix for the number of data points. \n",
    "    print(X.shape, Y.shape)\n",
    "    I = IdentityMatrix(X.shape[1])    \n",
    "    step1 = np.dot(X.T,X)\n",
    "    step2 = np.add(step1, np.dot(lambda_factor, I))\n",
    "    step3 = np.linalg.inv(step2)\n",
    "    theta = np.dot(step3, np.dot(X.T,Y))\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "X = train_x[:n]\n",
    "Y = train_y[:n]\n",
    "theta_0 = np.ones((n,1))\n",
    "print(X.shape, theta_0.shape)\n",
    "X = np.hstack([X,theta_0]) \n",
    "theta = closed_form(X, Y, 0.1)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################\n",
    "# 2. Linear Regression with Closed Form Solution\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error on Linear Regression\n",
    "\n",
    "Apply the linear regression model on the test set. For classification purpose, you decide to round the predicted label into numbers 0-9.\n",
    "\n",
    "**Note:** For this project we will be looking at the error rate defined as the fraction of labels that don't match the target labels, also known as the \"gold labels\" or ground truth. (In other context, you might want to consider other performance measures such as precision and recall, which we have not discussed in this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression_on_MNIST(lambda_factor=1):\n",
    "    \"\"\"\n",
    "    Trains linear regression, classifies test data, computes test error on test set\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    train_x_bias = np.hstack([np.ones([train_x.shape[0], 1]), train_x])\n",
    "    test_x_bias = np.hstack([np.ones([test_x.shape[0], 1]), test_x])\n",
    "    theta = closed_form(train_x_bias, train_y, lambda_factor)\n",
    "    test_error = compute_test_error_linear(test_x_bias, test_y, theta)\n",
    "    return test_error\n",
    "\n",
    "\n",
    "# Don't run this until the relevant functions in linear_regression.py have been fully implemented.\n",
    "print('Linear Regression test_error =', run_linear_regression_on_MNIST(lambda_factor=1))\n",
    "print('Linear Regression test_error =', run_linear_regression_on_MNIST(lambda_factor=0.1))\n",
    "print('Linear Regression test_error =', run_linear_regression_on_MNIST(lambda_factor=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that no matter what lambda factor is used, the test error is large. What can be wrong in this approach?\n",
    "\n",
    "Which of the following can be true in this approach:\n",
    "\n",
    "- Gradient descent should be used instead of the closed form solution. - TRUE\n",
    "\n",
    "- The loss function related to the closed-form solution is inadequate for this problem.\n",
    "\n",
    "- Regularization should not be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################\n",
    "# 3. Support Vector Machine\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def one_vs_rest_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for binary classifciation\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point\n",
    "    \"\"\"\n",
    "    clf = LinearSVC(random_state=0, C=0.1)\n",
    "    clf.fit(train_x, train_y)\n",
    "    pred_test_y = clf.predict(test_x)\n",
    "    return pred_test_y\n",
    "    # raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def compute_test_error_svm(test_y, pred_test_y):\n",
    "    return 1 - np.mean(pred_test_y == test_y)\n",
    "    \n",
    "def run_svm_one_vs_rest_on_MNIST():\n",
    "    \"\"\"\n",
    "    Trains svm, classifies test data, computes test error on test set\n",
    "\n",
    "    Returns:\n",
    "        Test error for the binary svm\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    train_y[train_y != 0] = 1\n",
    "    test_y[test_y != 0] = 1\n",
    "    pred_test_y = one_vs_rest_svm(train_x, train_y, test_x)\n",
    "    test_error = compute_test_error_svm(test_y, pred_test_y)\n",
    "    return test_error\n",
    "\n",
    "\n",
    "print('SVM one vs. rest test_error:', run_svm_one_vs_rest_on_MNIST())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for multiclass classifciation using a one-vs-rest strategy\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (int) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (int) for each test data point\n",
    "    \"\"\"\n",
    "    clf = LinearSVC(random_state=0, C=0.1)\n",
    "    clf.fit(train_x, train_y)\n",
    "    pred_test_y = clf.predict(test_x)\n",
    "    return pred_test_y\n",
    "\n",
    "def run_multiclass_svm_on_MNIST():\n",
    "    \"\"\"\n",
    "    Trains svm, classifies test data, computes test error on test set\n",
    "\n",
    "    Returns:\n",
    "        Test error for the binary svm\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    pred_test_y = multi_class_svm(train_x, train_y, test_x)\n",
    "    print(pred_test_y[:10])\n",
    "    test_error = compute_test_error_svm(test_y, pred_test_y)\n",
    "    return test_error\n",
    "\n",
    "\n",
    "print('Multiclass SVM test_error:', run_multiclass_svm_on_MNIST())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################\n",
    "# 4. Multinomial (Softmax) Regression and Gradient Descent\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import *\n",
    "import softmax\n",
    "theta = np.array([\n",
    " [ 0,  1,  2,  3,  4],\n",
    " [ 5,  6,  7,  8,  9],\n",
    " [10, 11, 12, 13, 14],\n",
    " [15, 16, 17, 18, 19],\n",
    " [20, 21, 22, 23, 24],\n",
    " [25, 26, 27, 28, 29],\n",
    " [30, 31, 32, 33, 34]\n",
    "])\n",
    "X = np.array([\n",
    " [ 0,  1,  2,  3,  4],\n",
    " [ 5,  6,  7,  8,  9],\n",
    " [10, 11, 12, 13, 14]\n",
    "])\n",
    "t= 0.2\n",
    "Expected = [[0., 0., 0.],\n",
    " [0., 0., 0.],\n",
    " [0., 0., 0.],\n",
    " [0., 0., 0.],\n",
    " [0., 0., 0.],\n",
    " [0., 0., 0.],\n",
    " [1., 1., 1.]]\n",
    "softmax.compute_probabilities(X, theta, t)\n",
    "# check_compute_probabilities()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import *\n",
    "check_run_gradient_descent_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from softmax import *\n",
    "\n",
    "def run_softmax_on_MNIST(temp_parameter=1):\n",
    "    \"\"\"\n",
    "    Trains softmax, classifies test data, computes test error, and plots cost function\n",
    "\n",
    "    Runs softmax_regression on the MNIST training set and computes the test error using\n",
    "    the test set. It uses the following values for parameters:\n",
    "    alpha = 0.3\n",
    "    lambda = 1e-4\n",
    "    num_iterations = 150\n",
    "\n",
    "    Saves the final theta to ./theta.pkl.gz\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    theta, cost_function_history = softmax_regression(train_x, train_y, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n",
    "    plot_cost_function_over_time(cost_function_history)\n",
    "    test_error = compute_test_error(test_x, test_y, theta, temp_parameter)\n",
    "    # Save the model parameters theta obtained from calling softmax_regression to disk.\n",
    "    write_pickle_data(theta, \"./theta.pkl.gz\")\n",
    "\n",
    "    # TODO: add your code here for the \"Using the Current Model\" question in tab 6.\n",
    "    #      and print the test_error_mod3\n",
    "    train_y_mod3, test_y_mod3 = update_y(train_y,test_y)\n",
    "    test_error = compute_test_error_mod3(test_x,test_y_mod3, theta, temp_parameter)\n",
    "    return test_error\n",
    "\n",
    "\n",
    "print('softmax test_error=', run_softmax_on_MNIST(temp_parameter=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#######################################################################\n",
    "# 6. Changing Labels\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from linear_regression import *\n",
    "from softmax import *\n",
    "from features import *\n",
    "from kernel import *\n",
    "\n",
    "\n",
    "def run_softmax_on_MNIST_mod3(temp_parameter=1):\n",
    "    \"\"\"\n",
    "    Trains Softmax regression on digit (mod 3) classifications.\n",
    "\n",
    "    See run_softmax_on_MNIST for more info.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    train_y_mod3, test_y_mod3 = update_y(train_y, test_y)\n",
    "    theta, cost_function_history = softmax_regression(train_x, train_y_mod3, temp_parameter, alpha= 0.3, lambda_factor = 1.0e-4, k = 10, num_iterations = 150)\n",
    "    plot_cost_function_over_time(cost_function_history)\n",
    "    test_error = compute_test_error(test_x, test_y_mod3, theta, temp_parameter)\n",
    "    return test_error\n",
    "    # raise NotImplementedError\n",
    "\n",
    "\n",
    "# # TODO: Run run_softmax_on_MNIST_mod3(), report the error rate\n",
    "print('softmax test_error(t=1)=', run_softmax_on_MNIST_mod3(temp_parameter=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################\n",
    "# 7. Classification Using Manually Crafted Features\n",
    "#######################################################################\n",
    "\n",
    "## Dimensionality reduction via PCA ##\n",
    "\n",
    "**TODO:** First fill out the PCA functions in features.py as the below code depends on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components = 18\n",
    "\n",
    "###Correction note:  the following 4 lines have been modified since release.\n",
    "train_x_centered, feature_means = center_data(train_x)\n",
    "pcs = principal_components(train_x_centered)\n",
    "train_pca = project_onto_PC(train_x, pcs, n_components, feature_means)\n",
    "test_pca = project_onto_PC(test_x, pcs, n_components, feature_means)\n",
    "\n",
    "# train_pca (and test_pca) is a representation of our training (and test) data\n",
    "# after projecting each example onto the first 18 principal components.\n",
    "# # TODO: Train your softmax regression model using (train_pca, train_y)\n",
    "# #       and evaluate its accuracy on (test_pca, test_y).\n",
    "theta, cost_function_history = softmax_regression(train_pca, train_y, temp_parameter=1, alpha= 0.3, lambda_factor = 1.0e-4, k = 10, num_iterations = 150)\n",
    "# plot_cost_function_over_time(cost_function_history)\n",
    "test_error = compute_test_error(test_pca, test_y, theta, temp_parameter=1)\n",
    "print(\"Test error with 18-dim PCA representation:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Use the plot_PC function in features.py to produce scatterplot\n",
    "# #       of the first 100 MNIST images, as represented in the space spanned by the\n",
    "# #       first 2 principal components found above.\n",
    "plot_PC(train_x[range(0, 100), ], pcs, train_y[range(0, 100)], feature_means)#feature_means added since release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Use the reconstruct_PC function in features.py to show\n",
    "# #       the first and second MNIST images as reconstructed solely from\n",
    "# #       their 18-dimensional principal component representation.\n",
    "# #       Compare the reconstructed images with the originals.\n",
    "firstimage_reconstructed = reconstruct_PC(train_pca[0, ], pcs, n_components, train_x, feature_means)#feature_means added since release\n",
    "plot_images(firstimage_reconstructed)\n",
    "plot_images(train_x[0, ])\n",
    "\n",
    "secondimage_reconstructed = reconstruct_PC(train_pca[1, ], pcs, n_components, train_x, feature_means)#feature_means added since release\n",
    "plot_images(secondimage_reconstructed)\n",
    "plot_images(train_x[1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components = 10\n",
    "\n",
    "###Correction note:  the following 4 lines have been modified since release.\n",
    "train_x_centered, feature_means = center_data(train_x)\n",
    "pcs = principal_components(train_x_centered)\n",
    "train_pca10 = project_onto_PC(train_x, pcs, n_components, feature_means)\n",
    "test_pca10 = project_onto_PC(test_x, pcs, n_components, feature_means)\n",
    "\n",
    "# train_pca (and test_pca) is a representation of our training (and test) data\n",
    "# after projecting each example onto the first 18 principal components.\n",
    "# # TODO: Train your softmax regression model using (train_pca, train_y)\n",
    "# #       and evaluate its accuracy on (test_pca, test_y).\n",
    "theta, cost_function_history = softmax_regression(train_pca10, train_y, temp_parameter=1, alpha= 0.3, lambda_factor = 1.0e-4, k = 10, num_iterations = 150)\n",
    "# plot_cost_function_over_time(cost_function_history)\n",
    "test_error10 = compute_test_error(test_pca, test_y, theta, temp_parameter=1)\n",
    "print(\"Test error with 10-dim PCA representation:\", test_error10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Cubic Kernel ##\n",
    "# # TODO: Find the 10-dimensional PCA representation of the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "pcs = principal_components(train_x)\n",
    "train_pca10 = project_onto_PC(train_x, pcs, n_components,feature_means)\n",
    "test_pca10 = project_onto_PC(test_x, pcs, n_components,feature_means)\n",
    "\n",
    "\n",
    "theta, cost_function_history = softmax_regression(train_pca10, train_y, temp_parameter=1, alpha= 0.3, lambda_factor = 1.0e-4, k = 10, num_iterations = 150)\n",
    "# plot_cost_function_over_time(cost_function_history)\n",
    "test_error = compute_test_error(test_pca10, test_y, theta, temp_parameter=1)\n",
    "print(\"Test error with 18-dim PCA representation:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: First fill out cubicFeatures() function in features.py as the below code requires it.\n",
    "\n",
    "train_cube = cubic_features(train_pca10)\n",
    "test_cube = cubic_features(test_pca10)\n",
    "\n",
    "# # train_cube (and test_cube) is a representation of our training (and test) data\n",
    "# # after applying the cubic kernel feature mapping to the 10-dimensional PCA representations.\n",
    "\n",
    "\n",
    "# # TODO: Train your softmax regression model using (train_cube, train_y)\n",
    "# #       and evaluate its accuracy on (test_cube, test_y).\n",
    "theta, cost_function_history = softmax_regression(train_cube, train_y, temp_parameter=1, alpha= 0.3, lambda_factor = 1.0e-4, k = 10, num_iterations = 150)\n",
    "plot_cost_function_over_time(cost_function_history)\n",
    "test_error = compute_test_error(test_cube, test_y, theta, temp_parameter=1)\n",
    "print(\"Test error with 10-dim PCA with cubic features:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('LD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31c4a5216d1b70edb8ccadb0033ed466ad602c2eadb1aba86d4896a8d7ddf022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
